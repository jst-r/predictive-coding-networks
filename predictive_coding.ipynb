{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "from torch.nn import functional as F\n",
    "from typing import List\n",
    "\n",
    "%env PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "integration_step = 0.2\n",
    "learning_rate = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable setup\n",
    "s_in = t.tensor(\n",
    "    [-1.0, 1.0],\n",
    ")\n",
    "s_target = t.tensor(\n",
    "    [0.0, 1.0],\n",
    ")\n",
    "\n",
    "x = [\n",
    "    s_in,\n",
    "    t.zeros(\n",
    "        [5],\n",
    "    ),\n",
    "    t.zeros(\n",
    "        [3],\n",
    "    ),\n",
    "    s_target,\n",
    "]\n",
    "e = [None, None, None, None]  # type: List[t.Tensor | None]\n",
    "\n",
    "w = [\n",
    "    t.rand(\n",
    "        [2, 5],\n",
    "    ),\n",
    "    t.rand(\n",
    "        [5, 3],\n",
    "    ),\n",
    "    t.rand(\n",
    "        [3, 2],\n",
    "    ),\n",
    "]\n",
    "\n",
    "f = t.relu\n",
    "\n",
    "\n",
    "def df(x: t.Tensor):\n",
    "    return t.where(x > 0, t.ones_like(x), t.zeros_like(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4418, 0.2911, 0.4763])\n",
      "tensor([0.4468, 0.2948, 0.4806])\n",
      "tensor([0.4516, 0.2984, 0.4847])\n",
      "tensor([0.4563, 0.3018, 0.4888])\n",
      "tensor([0.4608, 0.3051, 0.4927])\n",
      "tensor([0.4651, 0.3084, 0.4964])\n",
      "tensor([0.4692, 0.3114, 0.5000])\n",
      "tensor([0.4732, 0.3143, 0.5035])\n",
      "tensor([0.4771, 0.3171, 0.5068])\n",
      "tensor([0.4808, 0.3197, 0.5100])\n",
      "tensor([0.4844, 0.3223, 0.5131])\n",
      "tensor([0.4878, 0.3247, 0.5160])\n",
      "tensor([0.4911, 0.3270, 0.5189])\n",
      "tensor([0.4942, 0.3292, 0.5215])\n",
      "tensor([0.4973, 0.3312, 0.5241])\n",
      "tensor([0.5002, 0.3332, 0.5266])\n",
      "tensor([0.5030, 0.3351, 0.5289])\n",
      "tensor([0.5056, 0.3368, 0.5312])\n",
      "tensor([0.5082, 0.3385, 0.5333])\n",
      "tensor([0.5107, 0.3401, 0.5354])\n",
      "tensor([0.5130, 0.3416, 0.5373])\n",
      "tensor([0.5153, 0.3431, 0.5392])\n",
      "tensor([0.5175, 0.3444, 0.5410])\n",
      "tensor([0.5195, 0.3457, 0.5427])\n",
      "tensor([0.5215, 0.3469, 0.5443])\n",
      "tensor([0.5235, 0.3481, 0.5458])\n",
      "tensor([0.5253, 0.3492, 0.5473])\n",
      "tensor([0.5271, 0.3502, 0.5487])\n",
      "tensor([0.5288, 0.3512, 0.5500])\n",
      "tensor([0.5304, 0.3521, 0.5513])\n",
      "tensor([0.5320, 0.3530, 0.5525])\n",
      "tensor([0.5335, 0.3537, 0.5536])\n",
      "tensor([0.5349, 0.3545, 0.5547])\n",
      "tensor([0.5363, 0.3552, 0.5558])\n",
      "tensor([0.5373, 0.3589, 0.5572])\n",
      "tensor([0.5326, 0.3598, 0.5540])\n",
      "tensor([0.5341, 0.3603, 0.5551])\n",
      "tensor([0.5396, 0.3624, 0.5593])\n",
      "tensor([0.5407, 0.3631, 0.5601])\n",
      "tensor([0.5418, 0.3637, 0.5609])\n",
      "tensor([0.5468, 0.3686, 0.5659])\n",
      "tensor([0.5474, 0.3689, 0.5663])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5478, 0.3692, 0.5667])\n",
      "tensor([0.5433, 0.3645, 0.5624])\n",
      "tensor([0.5442, 0.3652, 0.5631])\n",
      "tensor([0.5494, 0.3692, 0.5680])\n",
      "tensor([0.5498, 0.3696, 0.5683])\n",
      "tensor([0.5501, 0.3699, 0.5687])\n",
      "tensor([0.5504, 0.3703, 0.5689])\n",
      "tensor([0.5508, 0.3706, 0.5692])\n",
      "tensor([0.5511, 0.3709, 0.5695])\n",
      "tensor([0.5514, 0.3712, 0.5697])\n",
      "tensor([0.5517, 0.3715, 0.5700])\n",
      "tensor([0.5465, 0.3670, 0.5650])\n",
      "tensor([0.5471, 0.3674, 0.5654])\n",
      "tensor([0.5520, 0.3719, 0.5704])\n",
      "tensor([0.5522, 0.3722, 0.5706])\n",
      "tensor([0.5524, 0.3728, 0.5708])\n",
      "tensor([0.5518, 0.3733, 0.5705])\n",
      "tensor([0.5520, 0.3737, 0.5706])\n",
      "tensor([0.5520, 0.3737, 0.5706])\n",
      "tensor([0.5521, 0.3740, 0.5707])\n",
      "tensor([0.5523, 0.3743, 0.5709])\n",
      "tensor([0.5534, 0.3748, 0.5717])\n",
      "tensor([0.5534, 0.3750, 0.5717])\n",
      "tensor([0.5534, 0.3752, 0.5718])\n",
      "tensor([0.5529, 0.3754, 0.5713])\n",
      "tensor([0.5529, 0.3756, 0.5712])\n",
      "tensor([0.5550, 0.3761, 0.5734])\n",
      "tensor([0.5551, 0.3763, 0.5735])\n",
      "tensor([0.5552, 0.3766, 0.5736])\n",
      "tensor([0.5552, 0.3768, 0.5736])\n",
      "tensor([0.5553, 0.3770, 0.5737])\n",
      "tensor([0.5560, 0.3773, 0.5744])\n",
      "tensor([0.5560, 0.3775, 0.5745])\n",
      "tensor([0.5561, 0.3777, 0.5746])\n",
      "tensor([0.5561, 0.3780, 0.5747])\n",
      "tensor([0.5562, 0.3783, 0.5748])\n",
      "tensor([0.5563, 0.3787, 0.5749])\n",
      "tensor([0.5571, 0.3790, 0.5756])\n",
      "tensor([0.5572, 0.3793, 0.5757])\n",
      "tensor([0.5573, 0.3796, 0.5758])\n",
      "tensor([0.5574, 0.3800, 0.5760])\n",
      "tensor([0.5595, 0.3816, 0.5781])\n",
      "tensor([0.5596, 0.3817, 0.5781])\n",
      "tensor([0.5596, 0.3819, 0.5782])\n",
      "tensor([0.5597, 0.3821, 0.5782])\n",
      "tensor([0.5597, 0.3823, 0.5783])\n",
      "tensor([0.5597, 0.3825, 0.5783])\n",
      "tensor([0.5597, 0.3828, 0.5784])\n",
      "tensor([0.5598, 0.3830, 0.5784])\n",
      "tensor([0.5598, 0.3832, 0.5784])\n",
      "tensor([0.5598, 0.3830, 0.5785])\n",
      "tensor([0.5595, 0.3829, 0.5782])\n",
      "tensor([0.5596, 0.3832, 0.5783])\n",
      "tensor([0.5594, 0.3834, 0.5782])\n",
      "tensor([0.5596, 0.3837, 0.5783])\n",
      "tensor([0.5622, 0.3857, 0.5809])\n",
      "tensor([0.5622, 0.3859, 0.5810])\n",
      "tensor([0.5622, 0.3861, 0.5810])\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    for i in range(1, len(x) - 1):\n",
    "        x[i] = t.zeros_like(x[i])\n",
    "\n",
    "    for _ in range(32):\n",
    "        for i in range(0, len(x) - 1):\n",
    "            # Eq 11\n",
    "            e[i + 1] = x[i + 1] - f(x[i]).matmul(w[i])\n",
    "\n",
    "        for i in range(1, len(x) - 1):\n",
    "            # Eq 12\n",
    "            dx = integration_step * (-e[i] + t.dot(df(x[i]), t.matmul(w[i], e[i + 1])))\n",
    "            x[i] = x[i] + dx\n",
    "\n",
    "    for i in range(0, len(x) - 1):\n",
    "        dw = learning_rate * t.matmul(e[i + 1].unsqueeze(0).T, f(x[i]).unsqueeze(0))\n",
    "        w[i] = w[i] + dw.T\n",
    "\n",
    "    print(x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "def relu_gradient(x: t.Tensor):\n",
    "    return t.where(x > 0, t.ones_like(x), t.zeros_like(x))\n",
    "\n",
    "\n",
    "def sigmoid_gradient(x: t.Tensor):\n",
    "    return t.sigmoid(x) * (1 - t.sigmoid(x))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PCNConfig:\n",
    "    learning_rate = 0.005\n",
    "    integration_step = 0.01\n",
    "    n_relaxation_steps = 8\n",
    "\n",
    "\n",
    "class PCN:\n",
    "    def __init__(self, cfg: PCNConfig, shape: List[int], device=\"cuda:0\") -> None:\n",
    "        self.cfg = cfg\n",
    "        self.n_layers = len(shape)\n",
    "        self.shape = shape\n",
    "        assert (\n",
    "            self.n_layers >= 2\n",
    "        ), \"at least two dims are required (input dimension and output dimension)\"\n",
    "\n",
    "        self.device = t.device(device)\n",
    "        with self.device:\n",
    "            in_dim, *hidded, out_dim = shape\n",
    "\n",
    "            self.activations = [None for _ in range(self.n_layers)]  # type: List[t.Tensor | None]\n",
    "\n",
    "            self.errors = [None for _ in range(self.n_layers)]  # type: List[t.Tensor | None]\n",
    "\n",
    "            self.weights = [\n",
    "                t.nn.init.xavier_normal_(t.empty(from_, to))\n",
    "                for [from_, to] in zip(shape, shape[1:])\n",
    "            ]\n",
    "\n",
    "            self.activation_fn = t.sigmoid\n",
    "            self.activation_fn_gradient = sigmoid_gradient\n",
    "\n",
    "    def relaxation_step(self):\n",
    "        x = self.activations\n",
    "        e = self.errors\n",
    "        w = self.weights\n",
    "        f = self.activation_fn\n",
    "        df = self.activation_fn_gradient\n",
    "\n",
    "        # its done once per input\n",
    "        # for i in range(1, self.n_layers - 1):\n",
    "        #     x[i] = t.zeros_like(x[i])\n",
    "        print(x)\n",
    "        for i in range(0, self.n_layers - 1):\n",
    "            # Eq 11\n",
    "            e[i + 1] = x[i + 1] - w[i].matmul(f(x[i]))\n",
    "            print(e)\n",
    "\n",
    "        print(e)\n",
    "\n",
    "        for i in range(1, self.n_layers - 1):\n",
    "            # Eq 12\n",
    "            dx = -e[i] + t.dot(df(x[i]), t.matmul(w[i], e[i + 1]))\n",
    "            x[i] = x[i] + self.cfg.integration_step * dx\n",
    "\n",
    "    def weight_update(self):\n",
    "        x = self.activations\n",
    "        e = self.errors\n",
    "        w = self.weights\n",
    "        f = self.activation_fn\n",
    "\n",
    "        for i in range(0, len(x) - 1):\n",
    "            dw = t.matmul(e[i + 1].unsqueeze(0).T, f(x[i]).unsqueeze(0))\n",
    "            w[i] = w[i] + self.cfg.learning_rate * dw.T\n",
    "\n",
    "    def clear_activaitons(self, batch: int):\n",
    "        x = self.activations\n",
    "        with self.device:\n",
    "            for i in range(self.n_layers):\n",
    "                x[i] = t.zeros(batch, self.shape[i])\n",
    "\n",
    "    def forward(self, input_: t.Tensor):\n",
    "        self.clear_activaitons(input_.shape[0])\n",
    "        self.activations[0] = input_.to(self.device)\n",
    "\n",
    "        for _ in range(self.cfg.n_relaxation_steps):\n",
    "            self.relaxation_step()\n",
    "\n",
    "        return t.matmul(self.activations[-2], self.weights[-1])\n",
    "\n",
    "    def training_step(self, input_: t.Tensor, output: t.Tensor, n_relaxations_steps=32):\n",
    "        self.clear_activaitons(input_.shape[0])\n",
    "        self.activations[0] = input_.to(self.device)\n",
    "        self.activations[-1] = output.to(self.device)\n",
    "\n",
    "        for _ in range(self.cfg.n_relaxation_steps):\n",
    "            self.relaxation_step()\n",
    "\n",
    "        loss = F.mse_loss(\n",
    "            t.matmul(F.softmax(self.activations[-2]), self.weights[-1]), output\n",
    "        )\n",
    "\n",
    "        self.weight_update()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transformations for normalizing images\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.1307,), (0.3081,)\n",
    "        ),  # Mean and standard deviation of MNIST\n",
    "    ]\n",
    ")\n",
    "\n",
    "    # Load training and test sets\n",
    "train_set = datasets.MNIST(\"data\", train=True, download=True, transform=transform)\n",
    "test_set = datasets.MNIST(\"data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders for batching\n",
    "train_loader = DataLoader(train_set, batch_size=1, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " step 0\n",
      "[tensor([[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.7595,  1.1668,\n",
      "          2.3124,  2.8215,  1.7396,  0.1867, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0467,\n",
      "          1.0777,  2.3505,  2.7833,  2.7960,  2.7960,  2.7960,  2.7960,  1.7269,\n",
      "          0.1867, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242,  0.7850,  2.4015,  2.7960,  2.7960,  2.7960,  2.7960,\n",
      "          2.7960,  2.7960,  2.7960,  2.7960,  2.4778, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0595,  2.5160,  2.7960,\n",
      "          2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
      "          2.5542, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242,  2.2360,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
      "          0.7850,  0.1613,  2.3505,  2.7960,  2.6433,  0.1231, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242,  0.6704,  2.7451,  2.7960,  2.7960,\n",
      "          2.7960,  2.7960,  2.5287,  0.4413, -0.2969, -0.4242,  0.1995,  2.7960,\n",
      "          2.7960,  0.9759, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          0.4159,  2.6687,  2.7960,  2.7960,  2.7960,  1.5359, -0.0424, -0.4242,\n",
      "         -0.4242, -0.4242, -0.0042,  2.7960,  2.7960,  0.9759, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  1.8796,  2.7960,  1.9687,\n",
      "          0.4795, -0.2842, -0.4242, -0.4242, -0.4242, -0.4242,  0.5177,  2.7960,\n",
      "          2.7960,  0.9759, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.1824,  0.2249, -0.3097, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242,  1.6505,  2.7960,  2.7324,  0.5813, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.0296,  2.5415,  2.7960,\n",
      "          2.5542, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242,  0.4668,  2.7960,  2.7960,  2.5542, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242,  0.3395,  2.5033,  2.7960,  2.7324,\n",
      "          1.0904, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.1951,  0.1613,  0.7213,  1.8160,  1.8160,  1.3832,  1.0268,  0.1613,\n",
      "          1.9942,  2.7960,  2.7960,  2.1596, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242,  0.7341,  2.1469,  2.7960,  2.7960,  2.7960,\n",
      "          2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  0.8104,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.2333,  2.5924,\n",
      "          2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
      "          2.7960,  2.7960,  2.5797,  0.4413, -0.0296, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242,  1.0013,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
      "          2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
      "          2.5542, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  1.0013,  2.7960,\n",
      "          2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
      "          2.7960,  2.7960,  2.7960,  2.6433,  1.4596, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242,  1.0013,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
      "          2.7960,  2.7960,  2.7960,  2.6815,  1.4723,  2.0832,  2.6306,  0.7468,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.2249,  2.6306,\n",
      "          2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7833,  1.4723, -0.0042,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242,  0.2249,  1.1414,  1.7396,  2.7960,  2.7960,\n",
      "          1.4723,  0.7468, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]],\n",
      "       device='cuda:0'), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0'), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0'), tensor([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (784x32 and 1x784)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [117]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m label \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(label, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me[-2 norm]\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m.\u001b[39merrors[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mnorm())\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# print(\"x[-2] norm\", model.activations[-2].norm())\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# print(\"eigenvalue\", t.svd(model.weights[-1])[1])\u001b[39;00m\n",
      "Input \u001b[1;32mIn [115]\u001b[0m, in \u001b[0;36mPCN.training_step\u001b[1;34m(self, input_, output, n_relaxations_steps)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mn_relaxation_steps):\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelaxation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(\n\u001b[0;32m    101\u001b[0m     t\u001b[38;5;241m.\u001b[39mmatmul(F\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), output\n\u001b[0;32m    102\u001b[0m )\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_update()\n",
      "Input \u001b[1;32mIn [115]\u001b[0m, in \u001b[0;36mPCN.relaxation_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Eq 11\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     e[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m x[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[43mw\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(e)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(e)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (784x32 and 1x784)"
     ]
    }
   ],
   "source": [
    "model = PCN(PCNConfig(), [784, 32, 16, 10])\n",
    "\n",
    "\n",
    "for [i, [input_, label]] in enumerate(train_loader):\n",
    "    input_ = input_.flatten(-3, -1).to(model.device)\n",
    "    label = F.one_hot(label, num_classes=10).to(model.device)\n",
    "\n",
    "    print(f\"\\n step {i}\")\n",
    "    print(\"loss\", model.training_step(input_, label))\n",
    "    print(\"e[-2 norm]\", model.errors[-1].norm())\n",
    "    # print(\"x[-2] norm\", model.activations[-2].norm())\n",
    "    # print(\"eigenvalue\", t.svd(model.weights[-1])[1])\n",
    "\n",
    "    if i > 10000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m image, label \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m      2\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# batch in not handeled rn\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mweights[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36mPCN.forward\u001b[1;34m(self, input_)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m input_\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mn_relaxation_steps):\n\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelaxation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36mPCN.relaxation_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(e)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# Eq 11\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     e[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(e)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(e)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'Tensor'"
     ]
    }
   ],
   "source": [
    "image, label = batch\n",
    "image = image.flatten(0, -1) # batch in not handeled rn\n",
    "model.forward(image)\n",
    "model.weights[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0042, -0.0041, -0.0041, -0.0041, -0.0042, -0.0041, -0.0042, -0.0042,\n",
       "        -0.0041, -0.0042, -0.0042, -0.0041, -0.0042, -0.0042, -0.0042, -0.0042],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.activations[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0006, -0.0010, -0.0004, -0.0009, -0.0008, -0.0011, -0.0008, -0.0011,\n",
       "        -0.0007, -0.0009], device='cuda:0')"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.matmul(model.activations[-2], model.weights[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
